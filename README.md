# News AI Agent - MVP Setup Guide

Automatisierter News-Aggregator mit KI-Zusammenfassungen und t√§glichem Email-Digest.

## üéØ Features

- **RSS Feed Ingestion**: Automatisches Abrufen von News aus mehreren RSS-Feeds (alle 15 Minuten)
- **Deduplizierung**: Intelligente Erkennung und Filterung doppelter Artikel
- **KI-Zusammenfassungen**: Automatische Generierung von Kurz- und Langzusammenfassungen via OpenAI
- **T√§glicher Digest**: Sch√∂n formatierte HTML-Email mit allen neuen Artikeln (t√§glich um 8:00 Uhr)
- **Persistente Speicherung**: PostgreSQL-Datenbank f√ºr Article-Historie

## üìã Voraussetzungen

- Docker & Docker Compose installiert
- OpenAI API Key ([hier erhalten](https://platform.openai.com/api-keys))
- SMTP Email-Account (z.B. Gmail mit App-Passwort)

## üöÄ Installation & Setup

### 1. Environment Variables einrichten

Kopiere die `env.example` Datei und benenne sie in `.env` um:

```bash
cp env.example .env
```

Bearbeite die `.env` Datei und f√ºge deine Credentials ein:

```bash
# PostgreSQL (kannst du so lassen f√ºr lokales Testing)
POSTGRES_USER=n8n
POSTGRES_PASSWORD=dein_sicheres_passwort
POSTGRES_DB=news_agent

# n8n Web-Interface Zugang
N8N_USER=admin
N8N_PASSWORD=dein_n8n_passwort

# OpenAI API Key (ERFORDERLICH!)
OPENAI_API_KEY=sk-dein-openai-key-hier

# Email SMTP (ERFORDERLICH f√ºr Email-Digest)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=deine-email@gmail.com
SMTP_PASSWORD=dein-app-passwort
SMTP_FROM=News Agent <deine-email@gmail.com>

# Empf√§nger Email
RECIPIENT_EMAIL=deine-email@gmail.com

# FastAPI Configuration
API_KEY=dev-api-key-change-in-production
SECRET_KEY=dev-secret-key-32-chars-minimum
```

**Wichtig f√ºr Gmail:**
- Aktiviere 2-Faktor-Authentifizierung
- Erstelle ein [App-Passwort](https://myaccount.google.com/apppasswords) (nicht dein normales Passwort!)

### 2. Container starten

```bash
docker-compose up -d
```

Die Container starten jetzt:
- **FastAPI**: http://localhost:3000 (API + Docs)
- **n8n**: http://localhost:5678
- **PostgreSQL**: localhost:5432

Pr√ºfe den Status:
```bash
docker-compose ps
```

Logs ansehen:
```bash
docker-compose logs -f n8n
docker-compose logs -f postgres
```

### 3. n8n Dashboard √∂ffnen & Owner Account erstellen

√ñffne deinen Browser und gehe zu: **http://localhost:5678**

Beim ersten Start:
1. Erstelle deinen **Owner Account** (Email, Name, Passwort)
2. Die Workflows werden **automatisch importiert** ‚ú®

### 4. Credentials einrichten

n8n ben√∂tigt Credentials f√ºr externe Services. Gehe zu **Settings ‚Üí Credentials** und erstelle:

#### PostgreSQL Credentials
- Name: `PostgreSQL account`
- Host: `postgres`
- Database: `news_agent` (oder dein `POSTGRES_DB`)
- User: `n8n` (oder dein `POSTGRES_USER`)
- Password: dein `POSTGRES_PASSWORD`
- Port: `5432`

#### OpenAI Credentials
- Name: `OpenAI account`
- API Key: dein `OPENAI_API_KEY`

#### SMTP Credentials
- Name: `SMTP account`
- Host: `smtp.gmail.com` (oder dein `SMTP_HOST`)
- Port: `587`
- User: dein `SMTP_USER`
- Password: dein `SMTP_PASSWORD`
- Secure: `false` (f√ºr STARTTLS)

### 5. Workflows aktivieren

Der Hauptworkflow wurde **automatisch importiert** beim ersten Start! üéâ

Gehe zu **Workflows** und du siehst:
- **AI-news-agent** - Hauptworkflow (RSS Feed, Summarization, Email)

**Aktiviere den Workflow:**
1. √ñffne den Workflow
2. Klicke auf den **"Active"** Toggle oben rechts (muss gr√ºn sein)
3. Pr√ºfe, ob alle Nodes korrekt verbunden sind

**Wichtig:** Stelle sicher, dass der Workflow die richtigen Credentials verwendet (siehe Node-Settings).

**Hinweis:** Die REST API l√§uft jetzt als separater FastAPI-Service (nicht mehr als n8n Workflow).

## üß™ Testing

### Test 1: Datenbank-Verbindung pr√ºfen

```bash
docker exec -it news-agent-postgres psql -U n8n -d news_agent -c "SELECT * FROM articles;"
```

Du solltest mindestens einen Test-Artikel sehen.

### Test 2: RSS Ingestion manuell ausf√ºhren

1. √ñffne Workflow **"01 - RSS Feed Ingestion"**
2. Klicke auf **"Execute Workflow"** (Play-Button oben rechts)
3. Warte 10-20 Sekunden
4. Du solltest gr√ºne Checkmarks sehen und eine Meldung wie "Successfully ingested X new articles"

Pr√ºfe die Datenbank:
```bash
docker exec -it news-agent-postgres psql -U n8n -d news_agent -c "SELECT title, source, processed FROM articles ORDER BY fetched_at DESC LIMIT 5;"
```

### Test 3: Summarization manuell ausf√ºhren

1. √ñffne Workflow **"02 - Article Summarization"**
2. Klicke auf **"Execute Workflow"**
3. Warte (kann 20-60 Sekunden dauern, abh√§ngig von der Anzahl der Artikel)
4. Pr√ºfe Ergebnis: "Processed X articles (Y successful)"

Pr√ºfe die Summaries in der Datenbank:
```bash
docker exec -it news-agent-postgres psql -U n8n -d news_agent -c "SELECT title, summary_short FROM articles WHERE processed = TRUE LIMIT 3;"
```

### Test 4: Email Digest manuell senden

1. √ñffne Workflow **"03 - Daily Email Digest"**
2. Klicke auf **"Execute Workflow"**
3. Pr√ºfe dein Email-Postfach!
4. Du solltest eine sch√∂n formatierte HTML-Email erhalten

## üìä Workflow-√úbersicht

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AUTOMATISCHER ABLAUF                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   ‚è∞ Alle 15 Minuten
        ‚îÇ
        ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ RSS Ingestion   ‚îÇ  ‚Üí Holt neue Artikel
   ‚îÇ (Workflow 01)   ‚îÇ  ‚Üí Pr√ºft Duplikate
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Üí Speichert in DB
            ‚îÇ
            ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   PostgreSQL    ‚îÇ  ‚Üí Artikel unprocessed
   ‚îÇ   (Database)    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
   ‚è∞ Alle 5 Minuten ‚îÇ
            ‚îÇ
            ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Summarization   ‚îÇ  ‚Üí Holt 10 unprocessed
   ‚îÇ (Workflow 02)   ‚îÇ  ‚Üí OpenAI erstellt Summaries
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Üí Markiert als processed
            ‚îÇ
            ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   PostgreSQL    ‚îÇ  ‚Üí Artikel processed
   ‚îÇ   (Database)    ‚îÇ     aber unsent
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
   ‚è∞ T√§glich 8:00   ‚îÇ
            ‚îÇ
            ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Email Digest   ‚îÇ  ‚Üí Sammelt alle unsent
   ‚îÇ (Workflow 03)   ‚îÇ  ‚Üí Generiert HTML
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Üí Sendet Email
            ‚îÇ           ‚Üí Markiert als sent
            ‚ñº
       üìß Dein Postfach
```

## üîß Konfiguration anpassen

### Weitere RSS Feeds hinzuf√ºgen

1. √ñffne Workflow **"01 - RSS Feed Ingestion"**
2. Klicke auf das **"+"** Icon nach dem Schedule Trigger
3. F√ºge einen neuen **"RSS Feed Read"** Node hinzu
4. Gib die Feed-URL ein
5. Verbinde ihn mit dem **"Merge Feeds"** Node
6. Speichern!

**Beliebte News RSS Feeds:**
- TechCrunch: `https://feeds.feedburner.com/TechCrunch/`
- Hacker News: `https://hnrss.org/frontpage`
- Heise: `https://www.heise.de/rss/heise-atom.xml`
- Tagesschau: `https://www.tagesschau.de/xml/rss2/`
- The Verge: `https://www.theverge.com/rss/index.xml`

### Zeitplan √§ndern

**RSS Ingestion:**
- Standard: Alle 15 Minuten
- √Ñndern: √ñffne Workflow ‚Üí Schedule Trigger Node ‚Üí "Rule" anpassen

**Summarization:**
- Standard: Alle 5 Minuten
- √Ñndern: √ñffne Workflow ‚Üí Schedule Trigger Node ‚Üí "Rule" anpassen

**Email Digest:**
- Standard: T√§glich um 8:00 Uhr
- √Ñndern: √ñffne Workflow ‚Üí Schedule Trigger Node ‚Üí Cron Expression anpassen
- Beispiele:
  - `0 8 * * *` = T√§glich 8:00
  - `0 18 * * *` = T√§glich 18:00
  - `0 8 * * 1-5` = Werktags 8:00

### OpenAI Modell √§ndern

Im Workflow **"02 - Article Summarization"**:
- √ñffne den **"OpenAI Summarize"** Node
- Model: `gpt-4o-mini` (g√ºnstig, schnell) oder `gpt-4o` (bessere Qualit√§t, teurer)
- Temperature: `0.3` (konsistenter) bis `0.7` (kreativer)

## üõë Container stoppen/neustarten

```bash
# Stoppen
docker-compose down

# Neustarten
docker-compose up -d

# Neu bauen (bei √Ñnderungen)
docker-compose down
docker-compose up -d --build

# Alles l√∂schen (inkl. Daten!)
docker-compose down -v
```

## üêõ Troubleshooting

### Problem: "Cannot connect to PostgreSQL"
```bash
# Pr√ºfe ob Container l√§uft
docker-compose ps

# Pr√ºfe Logs
docker-compose logs postgres

# Neustart
docker-compose restart postgres
```

### Problem: "OpenAI API Error"
- Pr√ºfe ob API Key korrekt ist
- Pr√ºfe OpenAI Account Balance: https://platform.openai.com/usage
- Teste mit curl:
```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

### Problem: "Email wird nicht gesendet"
- Pr√ºfe SMTP Credentials in n8n
- F√ºr Gmail: Verwende **App-Passwort**, nicht normales Passwort
- Teste SMTP manuell:
```bash
docker exec -it news-agent-n8n sh
# Im Container:
telnet smtp.gmail.com 587
```

### Problem: "Workflows werden nicht automatisch ausgef√ºhrt"
- Pr√ºfe ob Workflows **aktiviert** sind (Toggle oben rechts)
- Schedule Trigger muss gr√ºn sein
- Pr√ºfe n8n Logs: `docker-compose logs n8n`

### Problem: "Zu viele Duplikate"
Die Deduplizierung basiert auf der **URL**. Wenn verschiedene Feeds leicht unterschiedliche URLs haben (z.B. mit Tracking-Parametern), werden sie als unterschiedlich erkannt.

**L√∂sung:** Erweitere die Deduplizierungs-Logik im Workflow 01.

## üåê REST API

Der News AI Agent bietet eine **production-ready REST API** mit FastAPI, um programmatisch mit dem System zu interagieren!

### Quick Start

1. **API startet automatisch!** ‚ú®
   - Starte Container: `docker compose up -d`
   - API l√§uft auf: http://localhost:3000
   - **Automatische Dokumentation**: http://localhost:3000/docs

2. **API Docs ansehen:**
   - **Swagger UI**: http://localhost:3000/docs (interaktiv, "Try it out" Buttons)
   - **ReDoc**: http://localhost:3000/redoc (alternative Dokumentation)
   - **OpenAPI JSON**: http://localhost:3000/openapi.json (f√ºr Postman/Insomnia)
   - **OpenAPI YAML**: http://localhost:3000/openapi.yaml (f√ºr Postman/Insomnia)

3. **Authentifizierung:**
   - Die meisten Endpoints ben√∂tigen einen API Key im Header: `X-API-Key`
   - Standard Dev Key: `dev-api-key-change-in-production` (in `.env` √§ndern!)
   - `/api/health` ist √∂ffentlich (keine Auth n√∂tig)

### API Endpoints

#### Feed Management
| Method | Endpoint | Beschreibung | Auth |
|--------|----------|-------------|------|
| POST | `/api/feeds` | RSS Feed erstellen | ‚úÖ |
| GET | `/api/feeds` | Alle Feeds auflisten | ‚úÖ |
| GET | `/api/feeds/{id}` | Feed abrufen | ‚úÖ |
| PUT | `/api/feeds/{id}` | Feed aktualisieren | ‚úÖ |
| DELETE | `/api/feeds/{id}` | Feed l√∂schen | ‚úÖ |
| PATCH | `/api/feeds/{id}/toggle` | Feed aktivieren/deaktivieren | ‚úÖ |

#### Article Management
| Method | Endpoint | Beschreibung | Auth |
|--------|----------|-------------|------|
| GET | `/api/articles` | Artikel auflisten (Filter: source, processed, sent, limit, offset) | ‚úÖ |
| GET | `/api/articles/{id}` | Artikel abrufen | ‚úÖ |
| DELETE | `/api/articles/{id}` | Artikel l√∂schen | ‚úÖ |

#### Workflow Actions
| Method | Endpoint | Beschreibung | Auth |
|--------|----------|-------------|------|
| POST | `/api/actions/scrape` | RSS Scraping triggern | ‚úÖ |
| POST | `/api/actions/summarize` | AI Summarization triggern | ‚úÖ |
| POST | `/api/actions/send-digest` | Email Digest senden | ‚úÖ |

#### System
| Method | Endpoint | Beschreibung | Auth |
|--------|----------|-------------|------|
| GET | `/api/health` | Health Check | ‚ùå |
| GET | `/api/stats` | System-Statistiken | ‚úÖ |

**Base URL:** `http://localhost:3000`

### Beispiele

```bash
# Health Check (keine Auth)
curl http://localhost:3000/api/health

# Feeds auflisten
curl -H "X-API-Key: dev-api-key-change-in-production" \
  http://localhost:3000/api/feeds

# Neuen Feed erstellen
curl -X POST \
  -H "X-API-Key: dev-api-key-change-in-production" \
  -H "Content-Type: application/json" \
  -d '{"name":"TechCrunch","url":"https://feeds.feedburner.com/TechCrunch/","language":"en","category":"tech"}' \
  http://localhost:3000/api/feeds

# Unverarbeitete Artikel abrufen
curl -H "X-API-Key: dev-api-key-change-in-production" \
  "http://localhost:3000/api/articles?processed=false&limit=10"

# RSS Scraping triggern
curl -X POST \
  -H "X-API-Key: dev-api-key-change-in-production" \
  -H "Content-Type: application/json" \
  -d '{}' \
  http://localhost:3000/api/actions/scrape

# Statistiken abrufen
curl -H "X-API-Key: dev-api-key-change-in-production" \
  http://localhost:3000/api/stats
```

### Postman/Insomnia Import

Die OpenAPI-Spezifikation kann in **JSON** oder **YAML** Format direkt importiert werden:

```bash
# Option 1: OpenAPI YAML herunterladen (empfohlen)
curl http://localhost:3000/openapi.yaml > openapi.yaml

# Option 2: OpenAPI JSON herunterladen
curl http://localhost:3000/openapi.json > openapi.json

# In Postman/Insomnia importieren
# File ‚Üí Import ‚Üí openapi.yaml (oder openapi.json)
```

**Tipp:** Die OpenAPI-Spec wird **automatisch generiert** von FastAPI. √Ñnderungen am Code sind sofort in der Spec sichtbar!

### Python Beispiel

```python
import requests

BASE_URL = "http://localhost:3000"
API_KEY = "dev-api-key-change-in-production"

headers = {"X-API-Key": API_KEY}

# Feeds abrufen
feeds = requests.get(f"{BASE_URL}/api/feeds", headers=headers).json()

# Neuen Feed erstellen
new_feed = requests.post(
    f"{BASE_URL}/api/feeds",
    headers=headers,
    json={
        "name": "Hacker News",
        "url": "https://hnrss.org/frontpage",
        "language": "en",
        "category": "tech"
    }
).json()

# Scraping triggern
requests.post(f"{BASE_URL}/api/actions/scrape", headers=headers)
```

## üìà N√§chste Schritte (Phase 2)

- [ ] Keyword-Filtering hinzuf√ºgen
- [ ] Mehrere Output-Kan√§le (Slack, Teams)
- [ ] Web-Dashboard f√ºr Artikel-Verwaltung
- [ ] Sentiment-Analyse
- [ ] Updater-Funktion (Quellen per Chat hinzuf√ºgen)
- [ ] Redis f√ºr Caching und Rate Limiting

## üìù Lizenz

Dieses Projekt ist ein MVP f√ºr pers√∂nliche/interne Nutzung.

---

**Viel Erfolg! üöÄ**

Bei Fragen: Pr√ºfe die Logs und die [n8n Dokumentation](https://docs.n8n.io/).
